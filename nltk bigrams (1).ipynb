{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JBxVD14j6W_s"
   },
   "outputs": [],
   "source": [
    "# !pip install -q python-docx feedparser bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ4TiZcwxIdv"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "important libraries\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CYRUDnbXDDZ4"
   },
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize, line_tokenize, sent_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import operator\n",
    "#nltk.download()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.util import bigrams, ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m787cCsXc3dR"
   },
   "outputs": [],
   "source": [
    "#funciton to save web data to text\n",
    "def save_to_txt(filename, data):\n",
    "   with open(filename, 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data from link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Jt2a6WM_MCG",
    "outputId": "18cbc019-18a0-410b-98aa-9cc7832d5e18"
   },
   "outputs": [],
   "source": [
    "# import feedparser\n",
    "# from bs4 import BeautifulSoup\n",
    "# #WEB SOURCE 1\n",
    "# llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "# entries = llog.get('entries')\n",
    "# all_content_1 = ''\n",
    "# for entry in entries:\n",
    "#   for content in entry.get('content'):\n",
    "#     all_content_1+=\"<br>\"+content.value\n",
    "\n",
    "# raw_text_from_web = BeautifulSoup(all_content_1, 'html.parser').get_text()\n",
    "\n",
    "# raw_text_from_web_filepath = 'raw_text_from_web.txt'\n",
    "# save_to_txt(raw_text_from_web_filepath, raw_text_from_web)\n",
    "\n",
    "# total_sents_in_web = sent_tokenize(raw_text_from_web)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "#WEB SOURCE 1\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "entries = llog.get('entries')\n",
    "all_content_1 = ''\n",
    "for entry in entries:\n",
    "  for content in entry.get('content'):\n",
    "    all_content_1+=\"<br>\"+content.value\n",
    "\n",
    "fireside = feedparser.parse(\"https://feeds.fireside.fm/bibleinayear/rss\")\n",
    "\n",
    "all_content_2 = ''\n",
    "for entry in fireside.get('entries'):\n",
    "  for content in entry.get('content'):\n",
    "    all_content_2+=\"<br>\"+content.value\n",
    "\n",
    "\n",
    "\n",
    "raw_text_from_web = BeautifulSoup(all_content_1+\"<br>\"+all_content_2, 'html.parser').get_text()\n",
    "raw_text_from_web_filepath = 'raw_text_from_web.txt'\n",
    "save_to_txt(raw_text_from_web_filepath, raw_text_from_web)\n",
    "\n",
    "total_sents_in_web = sent_tokenize(raw_text_from_web)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a wordlist and sentence wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZZOqGwEcpH2",
    "outputId": "4b8b12c0-4fcf-46ca-f4ce-2f7216fa3cbe"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import WordListCorpusReader\n",
    "\n",
    "#creating wrodlist\n",
    "word_list_corpus = WordListCorpusReader('.', [raw_text_from_web_filepath])\n",
    "\n",
    "# geting the text from wordlist\n",
    "wordlist_text = word_list_corpus.raw()\n",
    "\n",
    "\n",
    "#Find the words in a string using tokenizers\n",
    "wordlist_text_token = word_tokenize(wordlist_text)\n",
    "\n",
    "#Find the sentences in a string using tokenizers\n",
    "total_sents_worldlist = sent_tokenize(wordlist_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift one row and zip to get bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "-Pgi-cFk5XTE"
   },
   "outputs": [],
   "source": [
    "#here we are getting all bigrams of our sentences \n",
    "worldlist_bigrams = [e for x in total_sents_worldlist for e in zip(x.split(\" \")[:-1], x.split(\" \")[1:])]\n",
    "\n",
    "#here we calculate bigrams frequency\n",
    "frequienceB = nltk.FreqDist(worldlist_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('the', 'complete'): 553, ('complete', 'reading'): 553, ('For', 'the'): 549, ('visit', 'ascensionpress.com/bibleinayear.'): 538, ('reading', 'plan,'): 531, ('not', 'be'): 528, ('may', 'not'): 522, ('suitable', 'for'): 521, ('that', 'may'): 520, ('adult', 'themes'): 519, ...})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequienceB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Unigrams and their Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMynOuWEmxgR",
    "outputId": "21742da7-188b-4479-e84e-69ce4f417398"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('the',): 2883, ('and',): 1642, ('of',): 1493, ('to',): 1058, ('that',): 888, ('is',): 842, ('for',): 782, ('in',): 675, ('be',): 660, ('reading',): 651, ...})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here we are getting all bigrams of our sentences \n",
    "worldlist_unigram = [e for x in total_sents_worldlist for e in zip(x.split())]\n",
    "\n",
    "#here we calculate unigram frequency\n",
    "frequienceU = nltk.FreqDist(worldlist_unigram)\n",
    "frequienceU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating bigram count Table\n",
    "\n",
    "Defining a function which will create a bigram count table for a sentance in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1\n",
    "# Creating a function which will return all possible pairs  from word list\n",
    "# i.e if we pass ['apple','banna']\n",
    "# will return [('apple','apple'),('apple','banna'),('banna','apple'),('banna','banna')]\n",
    "\n",
    "def all_pairs(wordlist):\n",
    "    diff_pairs_list = list(itertools.permutations(wordlist,2))\n",
    "    same_pairs_list = []\n",
    "    for word in wordlist:\n",
    "        same_pair = (word,word)\n",
    "        same_pairs_list.append(same_pair)\n",
    "    all_pairs_list = diff_pairs_list + same_pairs_list\n",
    "    return all_pairs_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_count_table(sent):\n",
    "    words = word_tokenize(sent)\n",
    "    # removing smilar words\n",
    "    vocab = np.unique(words)\n",
    "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    " \n",
    "    # Create bigrams from all words in corpus\n",
    "#     bi_grams = list(set(bigrams(words)))\n",
    "    all_pairs_list = all_pairs(vocab)\n",
    " \n",
    "    \n",
    "    # initializing a zero metrix of size (number of unique words in the sentance)\n",
    "    zeros_matrix = np.zeros((len(vocab), len(vocab)),dtype=int)\n",
    " \n",
    "    # Loop through the bigrams taking the current and previous word,\n",
    "    # and the number of occurrences of the bigram.\n",
    "    for pair in all_pairs_list:\n",
    "        current = pair[1]\n",
    "        previous = pair[0]\n",
    "        \n",
    "        pos_current = vocab_index[current]\n",
    "        pos_previous = vocab_index[previous]\n",
    "        #the number of occurrences of the bigram in corpus.\n",
    "        pair_freq = frequienceB.get(pair)\n",
    "        \n",
    "        if pair_freq is None:\n",
    "            pair_freq = 0\n",
    "        zeros_matrix[pos_current][pos_previous] = pair_freq\n",
    "        \n",
    "    prob_matrix = np.matrix(zeros_matrix)\n",
    "    # return the matrix and the index\n",
    "    table = pd.DataFrame(prob_matrix,columns=vocab_index,index = vocab_index).T\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is</th>\n",
       "      <th>of</th>\n",
       "      <th>test</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>319</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      is  of  test  the  this\n",
       "is     0   0     0   28     2\n",
       "of     0   0     0  319    18\n",
       "test   0   0     0    1     0\n",
       "the    0   0     0    1     0\n",
       "this   5   0     0    0     0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'this is test of the the the'\n",
    "bigram_count_table(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram probability table\n",
    "\n",
    "This table is created for the purpose that, if we want we can create sentance probability from this table easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_prob_table(sent):\n",
    "   # tokanizing the sent\n",
    "    words = word_tokenize(sent)\n",
    "    # removing repeted words\n",
    "    vocab = np.unique(words)\n",
    "    # Creating a dictionary which contain the index of the word\n",
    "    vocab_index = {word: i for i, word in enumerate(vocab)}\n",
    " \n",
    "    # Create bigrams from all the words in the sentance\n",
    "    all_pairs_list = all_pairs(vocab)\n",
    " \n",
    "    \n",
    "    # initializing a metrix of size number of words in the sentance\n",
    "    zeros_matrix = np.zeros((len(vocab), len(vocab)))\n",
    " \n",
    "    # Loop through the bigrams taking the current and previous word,\n",
    "    for pair in all_pairs_list:\n",
    "        current = pair[1]\n",
    "        previous = pair[0]\n",
    "        \n",
    "        # Extracting position of the current and previous words from vocab_index\n",
    "        pos_current = vocab_index[current]\n",
    "        pos_previous = vocab_index[previous]\n",
    "        #Converting previous word into unigram to get its frequncy in the corpus from frequienceU\n",
    "        unigram = tuple(previous.split())\n",
    "        \n",
    "        # the number of occurrences of the bigram.\n",
    "        pair_freq = frequienceB.get(pair)\n",
    "        unigram_freq = frequienceU.get(unigram)\n",
    "        if pair_freq is None:\n",
    "            pair_freq = 0\n",
    "        if unigram_freq is None:\n",
    "            unigram_freq = 1 \n",
    "#         prob = round(bigram_freq /unigram_freq,4)\n",
    "        prob = pair_freq /unigram_freq\n",
    "        zeros_matrix[pos_current][pos_previous] = prob\n",
    "        \n",
    "    prob_matrix = np.matrix(zeros_matrix)\n",
    "    # return the matrix and the index\n",
    "    table = pd.DataFrame(prob_matrix,columns=vocab_index,index = vocab_index).T\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is test of the the the'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is</th>\n",
       "      <th>of</th>\n",
       "      <th>test</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033254</td>\n",
       "      <td>0.002375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.213664</td>\n",
       "      <td>0.012056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>0.042017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            is   of  test       the      this\n",
       "is    0.000000  0.0   0.0  0.033254  0.002375\n",
       "of    0.000000  0.0   0.0  0.213664  0.012056\n",
       "test  0.000000  0.0   0.0  0.142857  0.000000\n",
       "the   0.000000  0.0   0.0  0.000347  0.000000\n",
       "this  0.042017  0.0   0.0  0.000000  0.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob_table(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Sentance probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to calculate the probablity of a sentance in given corpus using bigram count table\n",
    "def calculate_sent_prob(sentance):\n",
    "    # Creating table uning the defined function\n",
    "    table = bigram_count_table(sentance)\n",
    "    \n",
    "    # Converting the sentance into tokens\n",
    "    wordlist_tokens = word_tokenize(sentance)\n",
    "    \n",
    "    # Creating bigrams of the sentance\n",
    "    wordlist_bigrams = list(bigrams(wordlist_tokens))\n",
    "    \n",
    "    # Creating an empty list to store each bigram prob\n",
    "    prob_list = []\n",
    "    \n",
    "    # Looping through the bigram list \n",
    "    for bi_gram in wordlist_bigrams:\n",
    "        previous = bi_gram[0]\n",
    "        current = bi_gram[1]\n",
    "        \n",
    "        # extracting bigram freq from the table\n",
    "        bigram_freq = table.loc[previous][current]\n",
    "        \n",
    "        # Converting previous word into unigram\n",
    "        unigram = tuple(previous.split())\n",
    "        # extracting the unigram frequency from unigram frequency in the corpus\n",
    "        unigram_freq = frequienceU.get(unigram)\n",
    "        \n",
    "        # if unigram is not availible in the corpus we will assign it to 1, tp avoid Nonetype devision\n",
    "        # Note assigning it to wont effect the probablity of the sentance\n",
    "        if unigram_freq is None:\n",
    "            unigram_freq = 1 \n",
    "        \n",
    "        # Calculating the current bigram prob in the corpus\n",
    "        \n",
    "        bigram_prob = bigram_freq/unigram_freq\n",
    "        \n",
    "        prob_list.append(bigram_prob)\n",
    "    sentance_prob = np.prod(prob_list)\n",
    "    return sentance_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0013972334777141262"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1 = 'this is the'\n",
    "calculate_sent_prob(test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in the corpus is less thats why most of the values are zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worng Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will result bigrams which has zero probablity in the prob table\n",
    "def incorrect_bigrams(sent):\n",
    "    incorrect_bigrms = []\n",
    "    prob_table = bigram_prob_table(sent)\n",
    "    tokens = word_tokenize(sent)\n",
    "    bi_grams = list(bigrams(tokens))\n",
    "    for bi in bi_grams:\n",
    "        if prob_table.loc[bi[0]][bi[1]] == 0:\n",
    "            incorrect_bigrms.append(bi)\n",
    "    \n",
    "    return incorrect_bigrms\n",
    "\n",
    "# will return those words which are not in the corpus or which are not a unigram\n",
    "def miss_spelled_words(sent):\n",
    "    inocrrect_unigrms = []\n",
    "    prob_table = bigram_prob_table(sent)\n",
    "    tokens = word_tokenize(sent)\n",
    "    uni_grams = list(ngrams(tokens,1))\n",
    "    for uni in uni_grams:\n",
    "        if uni not in frequienceU.keys():\n",
    "            inocrrect_unigrms.append(uni[0])\n",
    "    return inocrrect_unigrms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 'is'), ('a', 'boys')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_3 = 'you is a boys'\n",
    "incorrect_bigrams(test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tha', 'spelld', 'wrds']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_4 = \"which are tha spelld wrds in the test\"\n",
    "miss_spelled_words(test_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sugestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sugestions(sent):\n",
    "    incorrect_bigrms = incorrect_bigrams(sent)\n",
    "    for ic_bigram in incorrect_bigrms:\n",
    "        bigram_dic = {}\n",
    "        for key,value in frequienceB.items():\n",
    "            if key[0] == ic_bigram[0]:\n",
    "                bigram_dic[key] = value\n",
    "            \n",
    "        # Sorting the dic in Desending order , first key will have highest frequency\n",
    "        sorted_bigrams = dict( sorted(bigram_dic.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    \n",
    "        # slicing only the first three elements from the dictionary, which are top three most occuring \n",
    "        sugestion = dict(itertools.islice(sorted_bigrams.items(), 3))\n",
    "    \n",
    "        # printing sugestions\n",
    "        print(f'Suggested words for {ic_bigram} : {sugestion}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested words for ('we', 'is') : {('we', 'read'): 118, ('we', 'can'): 35, ('we', 'begin'): 30}\n",
      "Suggested words for ('is', 'testing') : {('is', 'advised.Fr.'): 343, ('is', 'advised.As'): 57, ('is', 'advised.In'): 35}\n",
      "Suggested words for ('testing', 'this') : {}\n"
     ]
    }
   ],
   "source": [
    "test_5 = 'we is testing this in'\n",
    "sugestions(test_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacing_words(sent):\n",
    "    # List of incorrect bigrams\n",
    "    incorrect_bigrms = incorrect_bigrams(sent)\n",
    "    # If the list is not empty\n",
    "    while len(incorrect_bigrms)>0:\n",
    "        # Initializing an empty dictionary\n",
    "        bigram_dic = {}\n",
    "        \n",
    "        for key,value in frequienceB.items():\n",
    "            if key[0] == incorrect_bigrms[0][0]:\n",
    "                bigram_dic[key] = value\n",
    "            \n",
    "        # Sorting in Desending order , first bigram will have highest frequency\n",
    "        sorted_bigrams = sorted(bigram_dic.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        \n",
    "        wrong_word = incorrect_bigrms[0][1]\n",
    "        correct_word = sorted_bigrams[0][0][1]\n",
    "        \n",
    "        sent  = sent.replace(wrong_word,correct_word)\n",
    "        \n",
    "        incorrect_bigrms = incorrect_bigrams(sent)\n",
    "    return sent\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can also be other problems which i dont know. but one problem i have noticed in my code is.\n",
    "If the first word of the input sentance is not in the corpus then this `replacing_words` function will through an error ... Because `bigram_dic` in the function will be empty and `sorted_bigrams` variable in the function block will also be empty and this line of code `correct_word = sorted_bigrams[0][0][1]` will try to do indexing in empty list which will trough error.\n",
    "I tried but due to my inexperience i could not handle this issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you is a boys'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you to a life'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacing_words(test_3)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "cmpe 409 hw2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "9a2332156e2d5f642ed3ad4388e4eebe758de39a02b8884b75663012fda81288"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
